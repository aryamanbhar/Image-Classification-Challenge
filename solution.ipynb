{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [05:03<00:00, 198.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing\n",
      "Standardizing features...\n",
      "Done standardizing\n",
      "Applying dimensionality reduction...\n",
      "Done dimensionality reduction\n",
      "Training the model...\n",
      "Model training complete\n",
      "Saving predictions to submit.csv...\n",
      "Predictions saved successfully.\n",
      "Total time taken: 1449.74 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from skimage.feature import graycomatrix, graycoprops, canny, corner_harris, hog\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Utility Functions\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load and return an image as a NumPy array.\"\"\"\n",
    "    try:\n",
    "        return plt.imread(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {image_path} not found.\")\n",
    "        return None\n",
    "\n",
    "def flatten_image(image):\n",
    "    \"\"\"Flatten the image array for analysis.\"\"\"\n",
    "    return image.flatten()\n",
    "\n",
    "def load_images(image_names, directory):\n",
    "    \"\"\"Load and flatten multiple images.\"\"\"\n",
    "    return Parallel(n_jobs=-1)(\n",
    "        delayed(lambda img_name: flatten_image(load_image(f\"{directory}/{img_name}\")))(img_name)\n",
    "        for img_name in image_names\n",
    "    )\n",
    "\n",
    "# Feature Extraction\n",
    "def extract_features(image):\n",
    "    \"\"\"Extract various features from an image.\"\"\"\n",
    "    temp_img = cv2.cvtColor(image.reshape(32, 32, 3), cv2.COLOR_RGB2GRAY)\n",
    "    temp_img = cv2.normalize(temp_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "    # GLCM with additional angles\n",
    "    glcm = graycomatrix(temp_img, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4])\n",
    "    glcm_props = [\n",
    "        graycoprops(glcm, prop).flatten()\n",
    "        for prop in ['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'energy', 'correlation']\n",
    "    ]\n",
    "\n",
    "    # Edge detection, corner detection, and HOG with adjusted parameters\n",
    "    edges = canny(temp_img, sigma=1).flatten()\n",
    "    corners = corner_harris(temp_img).flatten()\n",
    "    hog_feature = hog(temp_img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys').flatten()\n",
    "\n",
    "    return np.concatenate(glcm_props + [edges, corners, hog_feature])\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_data(train_csv, test_csv):\n",
    "    \"\"\"Preprocess the data by loading images and extracting features.\"\"\"\n",
    "    train_data = pd.read_csv(train_csv)\n",
    "    test_data = pd.read_csv(test_csv)\n",
    "\n",
    "    # Load and flatten images\n",
    "    train_images = load_images(train_data['im_name'], 'train_ims')\n",
    "    test_images = load_images(test_data['im_name'], 'test_ims')\n",
    "    train_data['image'] = train_images\n",
    "    test_data['image'] = test_images\n",
    "\n",
    "    # Combine train and test data\n",
    "    concat_data = pd.concat([train_data, test_data])\n",
    "\n",
    "    # Extract features for all images\n",
    "    sec_feature_list = Parallel(n_jobs=-1)(\n",
    "        delayed(extract_features)(concat_data['image'].iloc[i]) for i in tqdm(range(concat_data.shape[0]))\n",
    "    )\n",
    "    concat_data['sec_feature'] = sec_feature_list\n",
    "\n",
    "    # Combine secondary and image features\n",
    "    concat_sec_feature = np.vstack(concat_data['sec_feature'].to_numpy())\n",
    "    concat_image_feature = np.vstack(concat_data['image'].to_numpy())\n",
    "    concat_feature = np.hstack([concat_sec_feature, concat_image_feature])\n",
    "\n",
    "    return concat_feature, train_data, test_data\n",
    "\n",
    "# Standardization\n",
    "def standardize_features(concat_feature):\n",
    "    \"\"\"Standardize the features using StandardScaler.\"\"\"\n",
    "    return StandardScaler().fit_transform(concat_feature)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "def apply_dimensionality_reduction(concat_feature_sc, n_components=10):\n",
    "    \"\"\"Apply dimensionality reduction using PCA.\"\"\"\n",
    "    reducer = PCA(n_components=n_components)\n",
    "    return reducer.fit_transform(concat_feature_sc)\n",
    "\n",
    "# Model Training\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train a HistGradientBoostingClassifier model with optimized hyperparameters.\"\"\"\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.1,  # Slightly reduce learning rate for better generalization\n",
    "        max_iter=200,       # Increase iterations\n",
    "        max_depth=10,       # Allow deeper trees\n",
    "        min_samples_leaf=5, # Avoid overfitting by requiring a minimum number of samples in leaves\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Save Predictions\n",
    "def save_predictions(model, X_test, test_data, output_file):\n",
    "    \"\"\"Make predictions and save them to a CSV file.\"\"\"\n",
    "    preds = model.predict(X_test)\n",
    "    test_data['label'] = preds\n",
    "    test_data = test_data.drop(columns=['image'])\n",
    "    test_data.to_csv(output_file, index=False)\n",
    "\n",
    "# Main Workflow\n",
    "def main():\n",
    "    start = time.time()\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    concat_feature, train_data, test_data = preprocess_data(\"train.csv\", \"test.csv\")\n",
    "    print(\"Done preprocessing\")\n",
    "\n",
    "    print(\"Standardizing features...\")\n",
    "    concat_feature_sc = standardize_features(concat_feature)\n",
    "    print(\"Done standardizing\")\n",
    "\n",
    "    print(\"Applying dimensionality reduction...\")\n",
    "    embedding = apply_dimensionality_reduction(concat_feature_sc, n_components=10)\n",
    "    print(\"Done dimensionality reduction\")\n",
    "\n",
    "    # Combine original and reduced features\n",
    "    concat_feature_sc_umap = np.hstack([concat_feature_sc, embedding])\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model = train_model(concat_feature_sc_umap[:train_data.shape[0]], train_data['label'])\n",
    "    print(\"Model training complete\")\n",
    "\n",
    "    # Save predictions\n",
    "    print(\"Saving predictions to submit.csv...\")\n",
    "    test_features = concat_feature_sc[train_data.shape[0]:]\n",
    "    test_embedding = embedding[train_data.shape[0]:]\n",
    "    umap_x_embed = np.hstack([test_features, test_embedding])\n",
    "    save_predictions(model, umap_x_embed, test_data, \"submit.csv\")\n",
    "    print(\"Predictions saved successfully.\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Total time taken: {:.2f} seconds\".format(end - start))\n",
    "\n",
    "# Run the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
